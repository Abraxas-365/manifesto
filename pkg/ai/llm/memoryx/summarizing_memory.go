package memoryx

import (
	"context"
	"fmt"
	"strings"
	"sync"

	"github.com/Abraxas-365/manifesto/pkg/ai/llm"
)

const defaultSummarizationPrompt = `You are a conversation summarizer. Summarize the following conversation history concisely.
Preserve:
- Key decisions and their reasoning
- Important facts, names, and values
- Current state and progress on tasks
- Any unresolved questions or next steps
- Tool calls and their results (summarize what was done and the outcome)

Be concise but complete. Do not lose critical information. Write in third person.`

// SummarizingMemory wraps any Memory and automatically summarizes older messages
// when the estimated token count exceeds a threshold. It keeps the most recent
// messages verbatim and replaces everything before them with a rolling summary.
//
// The summary is generated by calling the provided LLM — this works with any
// provider (OpenAI, Anthropic, local models, etc.).
type SummarizingMemory struct {
	mu sync.Mutex

	inner     Memory
	llm       llm.LLM
	estimator TokenEstimator

	// MaxTokens is the threshold that triggers summarization.
	// When Messages() estimates more tokens than this, older messages are summarized.
	MaxTokens int

	// RecentToKeep is the number of recent messages to always keep verbatim.
	// These are never summarized. Defaults to 4 if zero.
	RecentToKeep int

	// SummarizationPrompt is the system prompt used when asking the LLM to summarize.
	// If empty, a sensible default is used.
	SummarizationPrompt string

	// SummarizationOptions are extra LLM options passed to the summarization call
	// (e.g. WithModel("gpt-4o-mini") to use a cheaper model for summaries).
	SummarizationOptions []llm.Option

	// OnSummarize is an optional callback invoked after a summarization occurs.
	// Useful for logging or metrics.
	OnSummarize func(originalCount int, summary string)
}

// SummarizingOption configures a SummarizingMemory.
type SummarizingOption func(*SummarizingMemory)

// WithMaxTokens sets the token threshold that triggers summarization.
func WithMaxTokens(n int) SummarizingOption {
	return func(s *SummarizingMemory) { s.MaxTokens = n }
}

// WithRecentToKeep sets how many recent messages are always kept verbatim.
func WithRecentToKeep(n int) SummarizingOption {
	return func(s *SummarizingMemory) { s.RecentToKeep = n }
}

// WithTokenEstimator sets a custom token estimator.
func WithTokenEstimator(e TokenEstimator) SummarizingOption {
	return func(s *SummarizingMemory) { s.estimator = e }
}

// WithSummarizationPrompt overrides the default summarization prompt.
func WithSummarizationPrompt(prompt string) SummarizingOption {
	return func(s *SummarizingMemory) { s.SummarizationPrompt = prompt }
}

// WithSummarizationOptions adds LLM options for the summarization call.
func WithSummarizationOptions(opts ...llm.Option) SummarizingOption {
	return func(s *SummarizingMemory) { s.SummarizationOptions = opts }
}

// WithOnSummarize sets a callback that fires after each summarization.
func WithOnSummarize(fn func(originalCount int, summary string)) SummarizingOption {
	return func(s *SummarizingMemory) { s.OnSummarize = fn }
}

// NewSummarizingMemory wraps an existing Memory with automatic summarization.
//
// Parameters:
//   - inner: the underlying memory to wrap (e.g. InMemoryMemory)
//   - llmClient: any LLM implementation used to generate summaries
//   - opts: configuration options
//
// Example:
//
//	base := memoryx.NewInMemoryMemory("You are a helpful assistant.")
//	mem := memoryx.NewSummarizingMemory(base, myLLM,
//	    memoryx.WithMaxTokens(8000),
//	    memoryx.WithRecentToKeep(6),
//	)
func NewSummarizingMemory(inner Memory, llmClient llm.LLM, opts ...SummarizingOption) *SummarizingMemory {
	sm := &SummarizingMemory{
		inner:     inner,
		llm:       llmClient,
		MaxTokens: 16000,
	}
	for _, opt := range opts {
		opt(sm)
	}
	if sm.estimator == nil {
		sm.estimator = &CharBasedEstimator{}
	}
	if sm.RecentToKeep <= 0 {
		sm.RecentToKeep = 4
	}
	return sm
}

func (s *SummarizingMemory) Add(message llm.Message) error {
	return s.inner.Add(message)
}

func (s *SummarizingMemory) Clear() error {
	return s.inner.Clear()
}

// Messages returns the message list, performing summarization if the token
// estimate exceeds MaxTokens. The returned slice always starts with the system
// prompt (if present), followed by an optional summary message, followed by
// the most recent verbatim messages.
func (s *SummarizingMemory) Messages() ([]llm.Message, error) {
	s.mu.Lock()
	defer s.mu.Unlock()

	messages, err := s.inner.Messages()
	if err != nil {
		return nil, err
	}

	estimated := s.estimator.EstimateTokens(messages)
	if estimated <= s.MaxTokens {
		return messages, nil
	}

	// Separate system prompt from conversation
	conversation := messages
	if len(messages) > 0 && messages[0].Role == llm.RoleSystem {
		conversation = messages[1:]
	}

	if len(conversation) <= s.RecentToKeep {
		// Not enough messages to summarize — return as is
		return messages, nil
	}

	// Split: [toSummarize... | recentToKeep...]
	splitIdx := len(conversation) - s.RecentToKeep
	toSummarize := conversation[:splitIdx]
	recent := conversation[splitIdx:]

	summary, err := s.summarize(toSummarize)
	if err != nil {
		// If summarization fails, return the original messages rather than erroring
		return messages, nil
	}

	if s.OnSummarize != nil {
		s.OnSummarize(len(toSummarize), summary)
	}

	// Rebuild the inner memory with compressed history.
	// Clear() preserves the system prompt, so we only add the summary + recent.
	if err := s.inner.Clear(); err != nil {
		return messages, nil
	}

	summaryMsg := llm.Message{
		Role:    llm.RoleUser,
		Content: fmt.Sprintf("[Previous conversation summary]\n%s", summary),
		Metadata: map[string]any{
			"summarized":        true,
			"original_messages": len(toSummarize),
		},
	}
	if err := s.inner.Add(summaryMsg); err != nil {
		return messages, nil
	}

	for _, msg := range recent {
		if err := s.inner.Add(msg); err != nil {
			return messages, nil
		}
	}

	return s.inner.Messages()
}

// summarize calls the LLM to produce a summary of the given messages.
func (s *SummarizingMemory) summarize(messages []llm.Message) (string, error) {
	prompt := s.SummarizationPrompt
	if prompt == "" {
		prompt = defaultSummarizationPrompt
	}

	// Format messages into a readable transcript
	var transcript strings.Builder
	for _, m := range messages {
		fmt.Fprintf(&transcript, "[%s]: %s\n", m.Role, m.Content)
		for _, tc := range m.ToolCalls {
			fmt.Fprintf(&transcript, "  -> tool_call(%s): %s\n", tc.Function.Name, tc.Function.Arguments)
		}
		if m.ToolCallID != "" {
			fmt.Fprintf(&transcript, "  (tool_call_id: %s)\n", m.ToolCallID)
		}
	}

	summarizeMessages := []llm.Message{
		llm.NewSystemMessage(prompt),
		llm.NewUserMessage(fmt.Sprintf("Summarize this conversation:\n\n%s", transcript.String())),
	}

	resp, err := s.llm.Chat(context.Background(), summarizeMessages, s.SummarizationOptions...)
	if err != nil {
		return "", fmt.Errorf("summarization LLM call failed: %w", err)
	}

	return resp.Message.Content, nil
}

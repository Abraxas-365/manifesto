// Package memoryx provides conversation memory implementations for LLM agents.
//
// The core abstraction is the [Memory] interface, which manages message history
// for agent conversations. Implementations can be composed (stacked) to build
// sophisticated memory systems.
//
// # Implementations
//
// [InMemoryMemory] is a simple in-memory message store. It is thread-safe and
// preserves the system prompt on Clear(). Use it as the base layer for other
// memory wrappers.
//
//	mem := memoryx.NewInMemoryMemory("You are a helpful assistant.")
//
// [SummarizingMemory] wraps any Memory and automatically summarizes older
// messages when the estimated token count exceeds a threshold. It keeps
// the most recent messages verbatim and replaces everything before them
// with a rolling summary generated by an LLM call. Provider-agnostic.
//
//	mem := memoryx.NewSummarizingMemory(base, myLLM,
//	    memoryx.WithMaxTokens(8000),
//	    memoryx.WithRecentToKeep(6),
//	)
//
// [ContextualMemory] wraps any Memory and augments it with semantic retrieval
// from a vector store. Every message is embedded and stored. On Messages(),
// it retrieves the most relevant past messages and injects them as context.
// Works with any embedding model and vector store backend.
//
//	mem := memoryx.NewContextualMemory(base, docStore,
//	    memoryx.WithContextTopK(5),
//	    memoryx.WithContextMinScore(0.7),
//	)
//
// # Composition
//
// Implementations are designed to be stacked:
//
//	base := memoryx.NewInMemoryMemory("You are a helpful assistant.")
//
//	// Layer 1: Summarize when context gets large
//	summarized := memoryx.NewSummarizingMemory(base, summaryLLM,
//	    memoryx.WithMaxTokens(8000),
//	    memoryx.WithRecentToKeep(6),
//	)
//
//	// Layer 2: Augment with semantically relevant past messages
//	contextual := memoryx.NewContextualMemory(summarized, docStore,
//	    memoryx.WithContextTopK(5),
//	)
//
//	agent := agentx.New(client, contextual)
//
// # Token Estimation
//
// [TokenEstimator] is an interface for estimating token counts.
// [CharBasedEstimator] provides a rough heuristic (~4 chars per token).
// Plug in a custom implementation (e.g. tiktoken) for more accuracy.
package memoryx
